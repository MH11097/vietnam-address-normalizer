"""
Test script to analyze fuzzy matching scores for spacing issues.
Demonstrates why "P LEIKU" vs "PLEIKU" gets low score.
"""

import sys
sys.path.insert(0, '/Users/minhhieu/Library/CloudStorage/OneDrive-Personal/Coding/Python/company/address_mapping')

from src.utils.matching_utils import (
    token_sort_ratio,
    levenshtein_normalized,
    jaccard_similarity,
    ensemble_fuzzy_score
)

try:
    from rapidfuzz import fuzz
    RAPIDFUZZ = True
except ImportError:
    from fuzzywuzzy import fuzz
    RAPIDFUZZ = False

print("=" * 100)
print(f"PHÃ‚N TÃCH CHI TIáº¾T: Táº¡i sao 'P LEIKU' vs 'PLEIKU' chá»‰ Ä‘Æ°á»£c {0.642:.3f}?")
print(f"Using: {'rapidfuzz' if RAPIDFUZZ else 'fuzzywuzzy'}")
print("=" * 100)

# Test cases
test_cases = [
    # Main case
    ("p leiku", "pleiku", "Case chÃ­nh: P vÃ  LEIKU tÃ¡ch rá»i"),

    # Similar spacing issues
    ("p lei ku", "pleiku", "Nhiá»u dáº¥u cÃ¡ch hÆ¡n"),
    ("pleiku", "pleiku", "Perfect match (baseline)"),
    ("p leik", "pleiku", "Thiáº¿u kÃ½ tá»± + dáº¥u cÃ¡ch"),

    # Other district patterns
    ("tx binh minh", "binh minh", "TX prefix"),
    ("q 1", "1", "Quáº­n sá»‘"),
    ("h long bien", "long bien", "Huyá»‡n prefix"),

    # Token variations
    ("ba dinh", "ba din", "1 kÃ½ tá»± khÃ¡c - NO space issue"),
    ("ba dinh", "badinh", "Thiáº¿u dáº¥u cÃ¡ch - reverse"),
    ("thanh pho p leiku", "pleiku", "Full form: 'thanh pho p leiku'"),
]

def analyze_case(s1: str, s2: str, description: str):
    """PhÃ¢n tÃ­ch chi tiáº¿t 1 test case"""
    print(f"\n{'â”€' * 100}")
    print(f"TEST: {description}")
    print(f"  Input 1: '{s1}'")
    print(f"  Input 2: '{s2}'")
    print(f"{'â”€' * 100}")

    # Calculate individual metrics
    token_score = token_sort_ratio(s1, s2)
    lev_score = levenshtein_normalized(s1, s2)
    jac_score = jaccard_similarity(s1, s2)
    ensemble_score = ensemble_fuzzy_score(s1, s2, log=False)

    # Token analysis
    tokens1 = set(s1.lower().strip().split())
    tokens2 = set(s2.lower().strip().split())
    token_intersection = tokens1 & tokens2
    token_union = tokens1 | tokens2

    # Character analysis
    chars1 = s1.replace(" ", "")
    chars2 = s2.replace(" ", "")

    print(f"\n1ï¸âƒ£  TOKEN SORT RATIO: {token_score:.3f} (weight: 50%)")
    print(f"    Tokens 1: {sorted(tokens1)}")
    print(f"    Tokens 2: {sorted(tokens2)}")
    print(f"    Intersection: {token_intersection} â†’ {len(token_intersection)} common")
    print(f"    Union: {token_union} â†’ {len(token_union)} total")

    # Show what token_sort_ratio actually does
    sorted_s1 = ' '.join(sorted(s1.lower().strip().split()))
    sorted_s2 = ' '.join(sorted(s2.lower().strip().split()))
    print(f"    After sorting: '{sorted_s1}' vs '{sorted_s2}'")
    print(f"    â†’ Uses Levenshtein on sorted tokens: {token_score:.3f}")

    print(f"\n2ï¸âƒ£  LEVENSHTEIN NORMALIZED: {lev_score:.3f} (weight: 30%)")
    print(f"    String 1: '{s1}' (length: {len(s1)})")
    print(f"    String 2: '{s2}' (length: {len(s2)})")

    # Calculate edit distance manually
    from Levenshtein import distance as lev_dist
    dist = lev_dist(s1.lower().strip(), s2.lower().strip())
    max_len = max(len(s1.lower().strip()), len(s2.lower().strip()))
    print(f"    Edit distance: {dist} operations")
    print(f"    Max length: {max_len}")
    print(f"    Score: 1 - ({dist}/{max_len}) = {lev_score:.3f}")

    # Without spaces
    chars_dist = lev_dist(chars1.lower(), chars2.lower())
    print(f"    ğŸ“Œ Náº¿u Bá» dáº¥u cÃ¡ch: '{chars1}' vs '{chars2}'")
    print(f"       Edit distance: {chars_dist}, Score: {1 - chars_dist/max(len(chars1), len(chars2)):.3f}")

    print(f"\n3ï¸âƒ£  JACCARD SIMILARITY: {jac_score:.3f} (weight: 20%)")
    print(f"    |A âˆ© B| = {len(token_intersection)}")
    print(f"    |A âˆª B| = {len(token_union)}")
    print(f"    Jaccard = {len(token_intersection)}/{len(token_union)} = {jac_score:.3f}")

    if jac_score == 0.0:
        print(f"    âš ï¸  JACCARD = 0 vÃ¬ KHÃ”NG cÃ³ token chung!")

    print(f"\n{'â•' * 100}")
    print(f"ğŸ“Š ENSEMBLE SCORE (Weighted Average):")
    print(f"    = {token_score:.3f} Ã— 0.5 + {lev_score:.3f} Ã— 0.3 + {jac_score:.3f} Ã— 0.2")
    print(f"    = {token_score * 0.5:.3f} + {lev_score * 0.3:.3f} + {jac_score * 0.2:.3f}")
    print(f"    = {ensemble_score:.3f}")

    # Threshold check
    threshold = 0.90
    status = "âœ… PASS" if ensemble_score >= threshold else "âŒ FAIL"
    print(f"\nğŸ¯ Threshold: {threshold:.2f} â†’ {status}")

    if ensemble_score < threshold:
        print(f"    Gap: {threshold - ensemble_score:.3f} (cáº§n thÃªm {(threshold - ensemble_score):.1%})")

    return ensemble_score


# Run all test cases
print("\n\n")
scores = []
for s1, s2, desc in test_cases:
    score = analyze_case(s1, s2, desc)
    scores.append((s1, s2, score, desc))


# Summary
print("\n\n")
print("=" * 100)
print("ğŸ“‹ SUMMARY: Táº¥t cáº£ test cases")
print("=" * 100)
print(f"{'Input 1':<25} {'Input 2':<15} {'Score':<10} {'Status':<10} Description")
print("â”€" * 100)

for s1, s2, score, desc in scores:
    status = "âœ… PASS" if score >= 0.90 else "âŒ FAIL"
    print(f"{s1:<25} {s2:<15} {score:.3f}      {status:<10} {desc}")

# Root cause analysis
print("\n\n")
print("=" * 100)
print("ğŸ” ROOT CAUSE ANALYSIS")
print("=" * 100)

print("""
Váº¤N Äá»€ CHÃNH: Current ensemble algorithm Dá»°A VÃ€O TOKENS

1. TOKEN SORT RATIO (50% weight) - Váº¤N Äá»€ Lá»šN NHáº¤T:
   â€¢ "p leiku" â†’ tokens: ["p", "leiku"]
   â€¢ "pleiku"  â†’ tokens: ["pleiku"]
   â€¢ Token overlap = 0 (KHÃ”NG cÃ³ token chung!)
   â€¢ Score tháº¥p vÃ¬ so sÃ¡nh "leiku p" vs "pleiku"

2. LEVENSHTEIN (30% weight) - OK nhÆ°ng bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi SPACE:
   â€¢ "p leiku" (7 chars) vs "pleiku" (6 chars)
   â€¢ Edit distance = 2 (xÃ³a "p " - 1 space + 1 char)
   â€¢ Score: 1 - 2/7 = 0.714
   â€¢ âœ… Náº¿u Bá» spaces: "pleiku" vs "pleiku" â†’ 1.000

3. JACCARD (20% weight) - FAIL hoÃ n toÃ n:
   â€¢ Set overlap = 0 (no common tokens)
   â€¢ Score = 0.0
   â€¢ KhÃ´ng Ä‘Ã³ng gÃ³p gÃ¬ vÃ o ensemble score

â¡ï¸  ENSEMBLE = 0.5 Ã— (token_sort) + 0.3 Ã— (lev) + 0.2 Ã— (0.0)
   â‰ˆ 0.5 Ã— 0.70 + 0.3 Ã— 0.71 + 0.0
   â‰ˆ 0.35 + 0.21 + 0.0
   â‰ˆ 0.56 - 0.70 (tÃ¹y implementation chi tiáº¿t)

VÃŒ SAO THáº¤P?
â€¢ Token-based metrics (70% weight) FAIL vá»›i spacing issues
â€¢ "p" vÃ  "leiku" Ä‘Æ°á»£c coi lÃ  2 tokens riÃªng biá»‡t
â€¢ So sÃ¡nh vá»›i "pleiku" (1 token) â†’ overlap = 0

CHá»ˆ CÃ“ Levenshtein (30% weight) lÃ m viá»‡c Ä‘Æ°á»£c, nhÆ°ng:
â€¢ Váº«n bá»‹ penalty vÃ¬ thÃªm 1 space
â€¢ KhÃ´ng Ä‘á»§ weight Ä‘á»ƒ Ä‘Æ°a score lÃªn >0.90
""")

print("\n" + "=" * 100)
print("ğŸ’¡ GIáº¢I PHÃP Äá»€ XUáº¤T")
print("=" * 100)
print("""
OPTION 1: ThÃªm CHARACTER-LEVEL matching (KhÃ´ng dÃ¹ng tokens)
   â€¢ ThÃªm metric: compare strings SAU KHI bá» spaces
   â€¢ "p leiku" â†’ "pleiku", "pleiku" â†’ "pleiku" â†’ 100% match!
   â€¢ Weight: 30% (giáº£m token_sort xuá»‘ng 40%)

OPTION 2: ThÃªm PARTIAL_RATIO / TOKEN_SET_RATIO
   â€¢ fuzz.partial_ratio() - substring matching
   â€¢ fuzz.token_set_ratio() - handles extra tokens better
   â€¢ CÃ³ thá»ƒ cho score cao hÆ¡n cho "p leiku" vs "pleiku"

OPTION 3: Pre-processing: Bá» single-character tokens
   â€¢ "p leiku" â†’ strip "p" â†’ "leiku" vs "pleiku"
   â€¢ TÄƒng token overlap
   â€¢ Risk: máº¥t info tá»« single-char tokens há»£p lá»‡

OPTION 4: Adaptive weights dá»±a trÃªn token count
   â€¢ Náº¿u 1 string cÃ³ 1 token, 1 string cÃ³ nhiá»u tokens
   â€¢ TÄƒng weight cho Levenshtein, giáº£m weight cho Jaccard
   â€¢ Intelligent scoring based on input characteristics
""")

print("\n" + "=" * 100)
